{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NormLorenz/ai-llm-gradio-rag/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s77sVqyNAiB"
      },
      "source": [
        "# Google Colab Jupyter Version\n",
        "Build a RAG pipeline from a file!\n",
        "\n",
        "# Project Documentation\n",
        "https://github.com/NormLorenz/ai-llm-gradio-rag"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To use uv with Google Colab and an existing uv.lock file, follow these steps: Install uv in Colab.\n",
        "First, install uv within your Colab environment.\n",
        "```ps\n",
        "    !pip install uv\n",
        "```\n",
        "\n",
        "Upload uv.lock and pyproject.toml.\n",
        "Ensure that both your uv.lock file and the corresponding pyproject.toml file (which uv.lock is based on) are present in your Colab environment. You can upload them directly to the Colab session or mount your Google Drive and access them from there. Synchronize the environment with uv sync.\n",
        "Navigate to the directory containing your pyproject.toml and uv.lock files (if not in the root) and run uv sync. This command will read the uv.lock file and install the exact versions of the dependencies specified within it into your Colab environment.\n",
        "```ps\n",
        "    !uv sync\n",
        "```\n",
        "\n",
        "Note: If you encounter issues related to environment variables in Colab, particularly with UV_CONSTRAINT or UV_BUILD_CONSTRAINT, you might need to unset them before running uv commands. This can be done by prefixing your commands or setting them in the environment:\n",
        "```python\n",
        "    # Option 1: Prefixing commands\n",
        "    !UV_CONSTRAINT= UV_BUILD_CONSTRAINT= uv sync\n",
        "\n",
        "    # Option 2: Setting environment variables globally (at the start of your notebook)\n",
        "    import os\n",
        "    os.environ[\"UV_CONSTRAINT\"] = \"\"\n",
        "    os.environ[\"UV_BUILD_CONSTRAINT\"] = \"\"\n",
        "```\n",
        "\n",
        "After successfully running uv sync, your Colab environment will be populated with the exact dependencies and versions specified in your uv.lock file, ensuring reproducibility."
      ],
      "metadata": {
        "id": "f9K8r6EwzFBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"UV_CONSTRAINT\"] = \"\"\n",
        "os.environ[\"UV_BUILD_CONSTRAINT\"] = \"\""
      ],
      "metadata": {
        "id": "m_me_yLt0Wwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch uv.lock and pyproject.toml files\n",
        "import os\n",
        "\n",
        "# Check and download uv.lock\n",
        "lock_file = 'uv.lock'\n",
        "if not os.path.exists(lock_file):\n",
        "    print(f\"Downloading {lock_file}...\")\n",
        "    ! wget -P / https://raw.githubusercontent.com/NormLorenz/ai-llm-gradio-rag/refs/heads/main/uv.lock\n",
        "\n",
        "# Check and download pyproject.toml\n",
        "toml_file = 'pyproject.toml'\n",
        "if not os.path.exists(toml_file):\n",
        "    print(f\"Downloading {toml_file}...\")\n",
        "    ! wget -P / https://raw.githubusercontent.com/NormLorenz/ai-llm-gradio-rag/refs/heads/main/pyproject.toml"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WNo-ilzys2Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv sync"
      ],
      "metadata": {
        "id": "I_5FzPYzvWb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxQ0TlyGO0sA"
      },
      "outputs": [],
      "source": [
        "# Declare imports\n",
        "import gradio as gr\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "import tempfile\n",
        "# from dotenv import load_dotenv\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVduZo68PSpi"
      },
      "outputs": [],
      "source": [
        "# Set API keys\n",
        "openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "pinecone_key = userdata.get(\"PINECONE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tofCiiOaoYDp"
      },
      "outputs": [],
      "source": [
        "# Set Pinecone index name\n",
        "PINECONE_INDEX_NAME = \"rag-qa-index\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVjivgoNoYDp"
      },
      "outputs": [],
      "source": [
        "# Define RAG Pipeline class\n",
        "class RAGPipeline:\n",
        "    \"\"\"Class to handle RAG pipeline operations\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize RAG pipeline components\"\"\"\n",
        "        self.embeddings = None\n",
        "        self.vectorstore = None\n",
        "        self.qa_chain = None\n",
        "        self.pc = None\n",
        "        self.index = None\n",
        "\n",
        "    def initialize_pinecone(self, api_key):\n",
        "        \"\"\"Initialize Pinecone client and create/connect to index\"\"\"\n",
        "        try:\n",
        "            self.pc = Pinecone(api_key=api_key)\n",
        "\n",
        "            # Check if index exists, if not create it\n",
        "            existing_indexes = [index.name for index in self.pc.list_indexes()]\n",
        "\n",
        "            if PINECONE_INDEX_NAME not in existing_indexes:\n",
        "                self.pc.create_index(\n",
        "                    name=PINECONE_INDEX_NAME,\n",
        "                    dimension=1536,  # OpenAI embeddings dimension\n",
        "                    metric='cosine',\n",
        "                    spec=ServerlessSpec(\n",
        "                        cloud='aws',\n",
        "                        region='us-east-1'\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            self.index = self.pc.Index(PINECONE_INDEX_NAME)\n",
        "            return \"‚úì Pinecone initialized successfully\"\n",
        "        except Exception as e:\n",
        "            return f\"‚úó Pinecone initialization failed: {str(e)}\"\n",
        "\n",
        "    def load_existing_vectorstore(self):\n",
        "        \"\"\"Load existing vector store from Pinecone if data exists\"\"\"\n",
        "        try:\n",
        "            # Initialize Pinecone connection\n",
        "            pinecone_status = self.initialize_pinecone(pinecone_key)\n",
        "            if \"failed\" in pinecone_status:\n",
        "                return False, \"‚úó Could not connect to Pinecone\"\n",
        "\n",
        "            # Check if index has data\n",
        "            if self.index.describe_index_stats().total_vector_count == 0:\n",
        "                return False, \"‚úó Vector database is empty. Please process a document first.\"\n",
        "\n",
        "            # Initialize embeddings\n",
        "            self.embeddings = OpenAIEmbeddings(openai_api_key=openai_key)\n",
        "\n",
        "            # Load existing vector store\n",
        "            self.vectorstore = PineconeVectorStore(\n",
        "                index=self.index,\n",
        "                embedding=self.embeddings\n",
        "            )\n",
        "\n",
        "            # Initialize QA chain\n",
        "            llm = ChatOpenAI(\n",
        "                model_name=\"gpt-4\",\n",
        "                temperature=0,\n",
        "                openai_api_key=openai_key\n",
        "            )\n",
        "\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=llm,\n",
        "                chain_type=\"stuff\",\n",
        "                retriever=self.vectorstore.as_retriever(\n",
        "                    search_kwargs={\"k\": 3}\n",
        "                ),\n",
        "                return_source_documents=True\n",
        "            )\n",
        "\n",
        "            vector_count = self.index.describe_index_stats().total_vector_count\n",
        "            return True, f\"‚úì Loaded existing vector database!\\n- Vectors in index: {vector_count}\\n- Ready for questions!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, f\"‚úó Error loading vector database: {str(e)}\"\n",
        "\n",
        "    def process_document(self, file, chunk_size, chunk_overlap):\n",
        "        \"\"\"Process uploaded document and store in Pinecone\"\"\"\n",
        "        try:\n",
        "            # Initialize APIs\n",
        "            os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "            pinecone_status = self.initialize_pinecone(pinecone_key)\n",
        "\n",
        "            if \"failed\" in pinecone_status:\n",
        "                return pinecone_status\n",
        "\n",
        "            # Save uploaded file temporarily\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.name)[1]) as tmp_file:\n",
        "                tmp_file.write(file.read() if hasattr(\n",
        "                    file, 'read') else open(file.name, 'rb').read())\n",
        "                tmp_path = tmp_file.name\n",
        "\n",
        "            # Load document based on file type\n",
        "            if file.name.endswith('.pdf'):\n",
        "                loader = PyPDFLoader(tmp_path)\n",
        "            elif file.name.endswith('.txt'):\n",
        "                loader = TextLoader(tmp_path)\n",
        "            else:\n",
        "                return \"‚úó Unsupported file format. Please upload PDF or TXT file.\"\n",
        "\n",
        "            documents = loader.load()\n",
        "\n",
        "            # Split documents into chunks\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=int(chunk_size),\n",
        "                chunk_overlap=int(chunk_overlap),\n",
        "                length_function=len\n",
        "            )\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "            # Initialize embeddings\n",
        "            self.embeddings = OpenAIEmbeddings(openai_api_key=openai_key)\n",
        "\n",
        "            # Create vector store\n",
        "            self.vectorstore = PineconeVectorStore.from_documents(\n",
        "                documents=chunks,\n",
        "                embedding=self.embeddings,\n",
        "                index_name=PINECONE_INDEX_NAME\n",
        "            )\n",
        "\n",
        "            # Initialize QA chain\n",
        "            llm = ChatOpenAI(\n",
        "                model_name=\"gpt-4\",\n",
        "                temperature=0,\n",
        "                openai_api_key=openai_key\n",
        "            )\n",
        "\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=llm,\n",
        "                chain_type=\"stuff\",\n",
        "                retriever=self.vectorstore.as_retriever(\n",
        "                    search_kwargs={\"k\": 3}\n",
        "                ),\n",
        "                return_source_documents=True\n",
        "            )\n",
        "\n",
        "            # Clean up temporary file\n",
        "            os.unlink(tmp_path)\n",
        "\n",
        "            return f\"‚úì Document processed successfully!\\n- File: {file.name}\\n- Chunks created: {len(chunks)}\\n- Ready for questions!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚úó Error processing document: {str(e)}\"\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        \"\"\"Answer question using RAG pipeline\"\"\"\n",
        "        if not self.qa_chain:\n",
        "            return \"‚ö† Please upload and process a document first!\"\n",
        "\n",
        "        if not question.strip():\n",
        "            return \"‚ö† Please enter a question!\"\n",
        "\n",
        "        try:\n",
        "            result = self.qa_chain.invoke({\"query\": question})\n",
        "\n",
        "            answer = result['result']\n",
        "            sources = result.get('source_documents', [])\n",
        "\n",
        "            # Format response with sources\n",
        "            response = f\"**Answer:**\\n{answer}\\n\\n\"\n",
        "\n",
        "            if sources:\n",
        "                response += \"**Sources:**\\n\"\n",
        "                for i, doc in enumerate(sources[:3], 1):\n",
        "                    content_preview = doc.page_content[:200] + \"...\" if len(\n",
        "                        doc.page_content) > 200 else doc.page_content\n",
        "                    response += f\"\\n{i}. {content_preview}\\n\"\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚úó Error answering question: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVg1Ev8loYDq"
      },
      "outputs": [],
      "source": [
        "# Initialize pipeline\n",
        "pipeline = RAGPipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72mnTyBsoYDr"
      },
      "outputs": [],
      "source": [
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"RAG Q&A Pipeline\") as demo:\n",
        "    gr.Markdown(\"# üìö Retrieval Augmented Generation (RAG)\")\n",
        "    gr.Markdown(\n",
        "        \"Upload a document (PDF or TXT) and ask questions about its content using AI-powered retrieval.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "\n",
        "            gr.Markdown(\"### üìÑ Document Upload\")\n",
        "            file_input = gr.File(file_types=[\".pdf\", \".txt\"])\n",
        "\n",
        "            with gr.Accordion(\"‚öôÔ∏è Advanced Settings\", open=False):\n",
        "                chunk_size = gr.Slider(\n",
        "                    minimum=100,\n",
        "                    maximum=2000,\n",
        "                    value=1000,\n",
        "                    step=100,\n",
        "                    label=\"Chunk Size\"\n",
        "                )\n",
        "                chunk_overlap = gr.Slider(\n",
        "                    minimum=0,\n",
        "                    maximum=500,\n",
        "                    value=200,\n",
        "                    step=50,\n",
        "                    label=\"Chunk Overlap\"\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                process_btn = gr.Button(\n",
        "                    \"üöÄ Process Document\", variant=\"primary\")\n",
        "                load_existing_btn = gr.Button(\n",
        "                    \"üìö Load Existing Data\", variant=\"secondary\")\n",
        "\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"Status\",\n",
        "                lines=5,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### üí¨ Ask Questions\")\n",
        "            question_input = gr.Textbox(\n",
        "                label=\"Your Question\",\n",
        "                placeholder=\"Ask anything about the uploaded document...\",\n",
        "                lines=3\n",
        "            )\n",
        "            ask_btn = gr.Button(\"üîç Get Answer\", variant=\"primary\")\n",
        "            answer_output = gr.Markdown(label=\"Answer\")\n",
        "\n",
        "            gr.Markdown(\"### üìù Example Questions\")\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    [\"What is the main topic of this document?\"],\n",
        "                    [\"Can you summarize the key points?\"],\n",
        "                    [\"What are the main conclusions?\"],\n",
        "                ],\n",
        "                inputs=question_input\n",
        "            )\n",
        "\n",
        "    # Event handlers\n",
        "    process_btn.click(\n",
        "        fn=pipeline.process_document,\n",
        "        inputs=[file_input, chunk_size, chunk_overlap],\n",
        "        outputs=status_output\n",
        "    )\n",
        "\n",
        "    load_existing_btn.click(\n",
        "        fn=lambda: pipeline.load_existing_vectorstore()[1],\n",
        "        inputs=[],\n",
        "        outputs=status_output\n",
        "    )\n",
        "\n",
        "    ask_btn.click(\n",
        "        fn=pipeline.answer_question,\n",
        "        inputs=question_input,\n",
        "        outputs=answer_output\n",
        "    )\n",
        "\n",
        "    question_input.submit(\n",
        "        fn=pipeline.answer_question,\n",
        "        inputs=question_input,\n",
        "        outputs=answer_output\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoCg9ixxoYDr"
      },
      "outputs": [],
      "source": [
        "# Launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=False, inbrowser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPiB06baQ6jF"
      },
      "source": [
        "#"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}